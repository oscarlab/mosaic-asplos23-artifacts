diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h
index a02c67291..2bb3e8254 100644
--- a/arch/x86/include/asm/pgtable.h
+++ b/arch/x86/include/asm/pgtable.h
@@ -1001,6 +1001,7 @@ extern void memblock_find_dma_reserve(void);
 void __init poking_init(void);
 unsigned long init_memory_mapping(unsigned long start,
 				  unsigned long end, pgprot_t prot);
+unsigned long init_ih_area(unsigned long long size);
 
 #ifdef CONFIG_X86_64
 extern pgd_t trampoline_pgd_entry;
diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c
index 740f3bdb3..df125f918 100644
--- a/arch/x86/kernel/setup.c
+++ b/arch/x86/kernel/setup.c
@@ -1084,6 +1084,10 @@ void __init setup_arch(char **cmdline_p)
 
 	trim_platform_memory_ranges();
 	trim_low_memory_range();
+	/*
+	 * Iceberg region is fixed to 4GB for now
+	 */
+	init_ih_area(0);
 
 	init_mem_mapping();
 
diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c
index e26f5c5c6..cf325c783 100644
--- a/arch/x86/mm/init.c
+++ b/arch/x86/mm/init.c
@@ -762,6 +762,26 @@ void __init init_mem_mapping(void)
 	early_memtest(0, max_pfn_mapped << PAGE_SHIFT);
 }
 
+unsigned long __ref init_ih_area(unsigned long long size)
+{
+	unsigned long long end;
+	unsigned long long base;
+	void *ptr;
+	int ret;
+#ifdef CONFIG_X86_64
+	end = (unsigned long long)max_pfn << PAGE_SHIFT;
+#else
+	end = max_low_pfn << PAGE_SHIFT;
+#endif
+	//FIXME Initialize (4GB - 8GB] region for iceberg
+	size = 1048576ull << PAGE_SHIFT;
+	base = 5368709120ull;
+	//base = end - size;
+	ret = memblock_reserve(base, size);
+
+	return 0;
+}
+
 /*
  * Initialize an mm_struct to be used during poking and a pointer to be used
  * during patching.
diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c
index b5a3fa403..6db4e9d60 100644
--- a/arch/x86/mm/init_64.c
+++ b/arch/x86/mm/init_64.c
@@ -33,6 +33,7 @@
 #include <linux/nmi.h>
 #include <linux/gfp.h>
 #include <linux/kcore.h>
+#include <linux/iceberg.h>
 
 #include <asm/processor.h>
 #include <asm/bios_ebda.h>
@@ -1290,6 +1291,8 @@ void __init mem_init(void)
 
 	/* this will put all memory onto the freelists */
 	memblock_free_all();
+
+	ih_init(1310720, 1835008);
 	after_bootmem = 1;
 	x86_init.hyper.init_after_bootmem();
 
diff --git a/include/linux/gfp.h b/include/linux/gfp.h
index 6e479e9c4..36dade451 100644
--- a/include/linux/gfp.h
+++ b/include/linux/gfp.h
@@ -504,6 +504,9 @@ static inline int arch_make_page_accessible(struct page *page)
 struct page *
 __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 							nodemask_t *nodemask);
+struct page *
+__alloc_pages_nodemask_ih(gfp_t gfp_mask, unsigned int order, int preferred_nid,
+		nodemask_t *nodemask, struct vm_area_struct *vma, unsigned long addr);
 
 static inline struct page *
 __alloc_pages(gfp_t gfp_mask, unsigned int order, int preferred_nid)
diff --git a/include/linux/iceberg.h b/include/linux/iceberg.h
new file mode 100644
index 000000000..5bfa9157e
--- /dev/null
+++ b/include/linux/iceberg.h
@@ -0,0 +1,46 @@
+#ifndef _LINUX_ICEBERG_H
+#define _LINUX_ICEBERG_H
+
+#include <linux/mm_types.h>
+
+#define IH_NR_BUCKETS 16384
+#define IH_BUCKET_SIZE 64
+#define IH_LEVEL1_SIZE 56
+#define IH_LEVEL2_SIZE 8
+
+#define FREQ_BITMAP_SIZE 8
+struct freq {
+	DECLARE_BITMAP(freq_bitmap, FREQ_BITMAP_SIZE);
+	int frequency[2];
+};
+struct bucket {
+	short int level1utilization; //number of pages used in level 1
+	short int level2utilization; //number of pages used in level 2
+	struct list_head freelistlevel1;
+	struct list_head freelistlevel2;
+	unsigned long begin_pfn;
+	int bucketno;
+	struct freq freqs[IH_BUCKET_SIZE];
+};
+
+struct ihkmem{
+	//struct spinlock lock;
+	int use_lock;
+	struct bucket buckets[IH_NR_BUCKETS];
+	struct task_struct *kicescand;
+	wait_queue_head_t kicescand_wait;
+	//Debug purpose only
+	int util, util_swap, util_max;
+};
+
+extern struct ihkmem ihkmem;
+
+int gethash(void *addr, struct vm_area_struct *vma, int seed, int mod, int base);
+void *ih_assign_bucket(void *addr, struct vm_area_struct *vma);
+void ih_init(unsigned long start_pfn, unsigned long end_pfn);
+struct page *ih_rmqueue(struct bucket *bucket, int level,
+			gfp_t gfp_flags,unsigned int alloc_flags);
+void free_ih_page(struct page *page);
+
+int run_kicescand(void);
+#endif /* _LINUX_ICEBERG_H */
diff --git a/include/linux/mm.h b/include/linux/mm.h
index 24b292fce..9727a8b84 100644
--- a/include/linux/mm.h
+++ b/include/linux/mm.h
@@ -2358,6 +2358,7 @@ extern void adjust_managed_page_count(struct page *page, long count);
 extern void mem_init_print_info(const char *str);
 
 extern void reserve_bootmem_region(phys_addr_t start, phys_addr_t end);
+extern void init_page_ih(unsigned long pfn);
 
 /* Free the reserved page into the buddy system, so it gets managed. */
 static inline void __free_reserved_page(struct page *page)
diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h
index 07d9acb5b..f7521043a 100644
--- a/include/linux/mm_types.h
+++ b/include/linux/mm_types.h
@@ -27,6 +27,9 @@
 struct address_space;
 struct mem_cgroup;
 
+extern unsigned long ih_callno_cnt;
+extern unsigned long ih_horizon;
+
 /*
  * Each physical page in the system has a struct page associated with
  * it to keep track of whatever it is we are using the page for at the
@@ -221,6 +224,8 @@ struct page {
 #ifdef LAST_CPUPID_NOT_IN_PAGE_FLAGS
 	int _last_cpupid;
 #endif
+	unsigned long ih_callno;
+	void *ih_bucket;
 } _struct_page_alignment;
 
 static inline atomic_t *compound_mapcount_ptr(struct page *page)
diff --git a/include/linux/swap.h b/include/linux/swap.h
index 55fe2f5b6..2eea7a7fb 100644
--- a/include/linux/swap.h
+++ b/include/linux/swap.h
@@ -370,6 +370,16 @@ extern int vm_swappiness;
 extern int remove_mapping(struct address_space *mapping, struct page *page);
 
 extern unsigned long reclaim_pages(struct list_head *page_list);
+
+enum {
+	IH_PAGE_EVICTABLE_NOW,
+	IH_PAGE_EVICTABLE_DELAYED,
+	IH_PAGE_UNEVICTABLE
+};
+
+extern int ih_test_page_evictable(struct page *page);
+extern int ih_evict_page(struct page *page);
+
 #ifdef CONFIG_NUMA
 extern int node_reclaim_mode;
 extern int sysctl_min_unmapped_ratio;
diff --git a/mm/Makefile b/mm/Makefile
index b6cd2fffa..9723e2bcc 100644
--- a/mm/Makefile
+++ b/mm/Makefile
@@ -38,7 +38,7 @@ mmu-y			:= nommu.o
 mmu-$(CONFIG_MMU)	:= highmem.o memory.o mincore.o \
 			   mlock.o mmap.o mmu_gather.o mprotect.o mremap.o \
 			   msync.o page_vma_mapped.o pagewalk.o \
-			   pgtable-generic.o rmap.o vmalloc.o ioremap.o
+			   pgtable-generic.o rmap.o vmalloc.o ioremap.o iceberg.o
 
 
 ifdef CONFIG_CROSS_MEMORY_ATTACH
diff --git a/mm/iceberg.c b/mm/iceberg.c
new file mode 100644
index 000000000..d5140dc8a
--- /dev/null
+++ b/mm/iceberg.c
@@ -0,0 +1,406 @@
+#include <linux/xxhash.h>
+#include <linux/iceberg.h>
+#include <linux/sysctl.h>
+#include <linux/mm.h>
+#include <linux/rmap.h>
+#include <linux/swap.h>
+#include <linux/kthread.h>
+#include <linux/freezer.h>
+#include <linux/random.h>
+
+struct ihkmem ihkmem;
+static int kicescand(void *p);
+struct task_struct *memtest;
+
+void __meminit ih_init(unsigned long start_pfn, unsigned long end_pfn)
+{
+	int i, j;
+	unsigned long pfn = start_pfn;
+	struct list_head *list;
+	struct page *page;
+	for(i = 0; i < IH_NR_BUCKETS; i++) {
+		ihkmem.buckets[i].begin_pfn = pfn;
+		ihkmem.buckets[i].bucketno = i;
+		list = &ihkmem.buckets[i].freelistlevel1;
+		INIT_LIST_HEAD(list);
+		for (j = 0; j < IH_LEVEL1_SIZE; j++) {
+			page = pfn_to_page(pfn);
+			init_page_ih(pfn);
+			page->ih_bucket = &ihkmem.buckets[i];
+			list_add_tail(&page->lru, list);
+			pfn++;
+		}
+		list = &ihkmem.buckets[i].freelistlevel2;
+		INIT_LIST_HEAD(list);
+		for (j = 0; j < IH_LEVEL2_SIZE; j++) {
+			page = pfn_to_page(pfn);
+			init_page_ih(pfn);
+			page->ih_bucket = &ihkmem.buckets[i];
+			list_add_tail(&page->lru, list);
+			pfn++;
+		}
+		ihkmem.buckets[i].level1utilization = 0;
+		ihkmem.buckets[i].level2utilization = 0;
+		ihkmem.util = 0;
+	}
+}
+
+void free_ih_page(struct page *page)
+{
+	struct bucket *bucket;
+	int slot;
+	struct list_head *list;
+	bucket = page->ih_bucket;
+	slot = page_to_pfn(page) - bucket->begin_pfn;
+	if (slot < 0 || slot >= IH_BUCKET_SIZE) {
+		panic("Invalid slot number\n");
+	}
+	bitmap_clear(bucket->freqs[slot].freq_bitmap,0,8);
+	if (slot < IH_LEVEL1_SIZE) {
+		list = &bucket->freelistlevel1;
+		bucket->level1utilization--;
+	} else {
+		list = &bucket->freelistlevel2;
+		bucket->level2utilization--;
+	}
+	list_add_tail(&page->lru, list);
+	page->ih_callno = 0;
+	ihkmem.util--;
+
+	return;
+}
+
+struct page *ih_rmqueue(struct bucket *bucket, int level,
+			gfp_t gfp_flags,unsigned int alloc_flags)
+{
+	struct page *page;
+	/* lock */
+	struct list_head *list = &bucket->freelistlevel1;
+	if (level == 2)
+		list = &bucket->freelistlevel2;
+	page = list_first_entry(list, struct page, lru);
+	list_del(&page->lru);
+
+	/* unlock */
+	return page;
+}
+
+// return hashed number with given seed in range [base, base+mod)
+int gethash(void *addr, struct vm_area_struct *vma, int seed, int mod, int base)
+{
+	uint64_t buf;
+	uint64_t hash;
+
+	//combine both to create key
+	buf = (uint64_t)addr>>12;
+	buf |= (uint64_t)vma->vm_mm->owner->pid << 36;
+
+	hash = xxh64(&buf, 8, seed);
+	return base+(hash%mod);
+}
+
+struct page *get_level1_bucket(void *v, void *vma, int *bn)
+{
+	struct page *page;
+	int seed1 = 2846245;
+	*bn = gethash(v,vma, seed1, IH_NR_BUCKETS, 0); //base = 0, mod = #buckets
+
+	/*
+	 * What to do if level 1 bucket is full.
+	 * 1: Check if there's any ghost page
+	 * 2: go check level 2
+	 */
+evict_l1:
+	if(ihkmem.buckets[*bn].level1utilization >= IH_LEVEL1_SIZE) {
+		unsigned long pfn, pfn_begin, pfn_end, pfn_evict;
+		unsigned long oldest_callno = ULONG_MAX;
+		struct page *scan_page;
+		pfn_begin = ihkmem.buckets[*bn].begin_pfn;
+		pfn_end = pfn_begin + IH_LEVEL1_SIZE;
+
+		for (pfn = pfn_begin; pfn < pfn_end; pfn++) {
+			scan_page = pfn_to_page(pfn);
+			if (scan_page->ih_callno < ih_horizon && scan_page->ih_callno < oldest_callno) {
+				oldest_callno = scan_page->ih_callno;
+				pfn_evict = pfn;
+			}
+		}
+		if (oldest_callno < ULONG_MAX) {
+			page = pfn_to_page(pfn_evict);
+			if (ihkmem.util_swap == 0) {
+				ihkmem.util_swap = ihkmem.util;
+			}
+			if (ih_evict_page(page) == 0) {
+			} else {
+				page->ih_callno = ih_callno_cnt++;
+				goto evict_l1;
+				panic("ICEBERG page eviction failed\n");
+			}
+		} else {
+			/* No ghost pages in this bucket */
+			return NULL;
+		}
+	}
+	page = ih_rmqueue(&ihkmem.buckets[*bn], 1, GFP_USER, 0);
+	ihkmem.buckets[*bn].level1utilization++;
+	ihkmem.util++;
+	if (ihkmem.util > ihkmem.util_max) {
+		ihkmem.util_max = ihkmem.util;
+	}
+
+	return page;
+}
+
+/* TODO: we don't need bucketnumbers as a parameter */
+int dchoicehashing(int d, void *v, void *vma, int totbuckets, int l1bn)
+{
+	int seed[] = { 8384032, 4392830, 6382738, 3994928, 9820291, 920194}; //seeds 0 to 5
+	int i;
+	int bucketnumber = -1;
+	int bucketnumbers[6];
+	int nlive[6];
+	int min_nlive;
+	unsigned long pfn, pfn_begin, pfn_end, pfn_evict;
+	int min_util = IH_LEVEL2_SIZE;
+	struct page *page;
+	unsigned long oldest_callno = ULONG_MAX;
+	unsigned long evict_addr;
+
+	if(d < 2) panic("make d = 2 min");
+	if(d > 6) panic("make d = 6 max");
+
+	for(i = 0; i < d; i++)
+	{
+		int hash_range = totbuckets/d + 1;
+		bucketnumbers[i] = gethash(v,vma, seed[i], hash_range, i * hash_range);
+		if(bucketnumbers[i] >= totbuckets)
+		{
+			bucketnumbers[i] = totbuckets - 1;
+		}
+		if (ihkmem.buckets[bucketnumbers[i]].level2utilization <= min_util) {
+			min_util = ihkmem.buckets[bucketnumbers[i]].level2utilization;
+			bucketnumber = bucketnumbers[i];
+		}
+	}
+	if (min_util != IH_LEVEL2_SIZE) {
+		return bucketnumber;
+	}
+evict_l2:
+	/*
+	 * All level2 buckets are full
+	 * 1: Check which bucket has most ghost pages
+	 * 2: Evict oldest ghost page from the bucket
+	 * 3: If there are no ghost pages in any of buckets
+	 * 3-1: evict oldest page among the level 1 bucket and 6 level 2 buckets
+	 */
+	// Count the number of live slots according to Horizon scheme
+	// TODO: Cache the number of live slots in bucket
+	min_nlive = IH_LEVEL2_SIZE;
+	oldest_callno = ULONG_MAX;
+	bucketnumber = -1;
+	for (i = 0; i < d; i++) {
+		nlive[i] = 0;
+		pfn_begin = ihkmem.buckets[bucketnumbers[i]].begin_pfn + IH_LEVEL1_SIZE;
+		pfn_end = pfn_begin + IH_LEVEL2_SIZE;
+		for (pfn = pfn_begin; pfn < pfn_end; pfn++) {
+			page = pfn_to_page(pfn);
+			if (page->ih_callno > ih_horizon) {
+				nlive[i]++;
+			}
+		}
+		if (nlive[i] < min_nlive) {
+			min_nlive = nlive[i];
+			bucketnumber = bucketnumbers[i];
+		}
+	}
+	if (bucketnumber != -1) {
+		/* 2: Evict oldest ghost page from the bucket */
+		pfn_begin = ihkmem.buckets[bucketnumber].begin_pfn + IH_LEVEL1_SIZE;
+		pfn_end = pfn_begin + IH_LEVEL2_SIZE;
+		for (pfn = pfn_begin; pfn < pfn_end; pfn++) {
+			page = pfn_to_page(pfn);
+			if (page->ih_callno < oldest_callno) {
+				oldest_callno = page->ih_callno;
+				pfn_evict = pfn;
+			}
+		}
+	} else {
+		/* 3: If there are no ghost pages in any of buckets */
+		/* New horizon */
+		/* 3-1: Find oldest page in level 1 */
+		pfn_begin = ihkmem.buckets[l1bn].begin_pfn;
+		pfn_end = pfn_begin + IH_LEVEL1_SIZE;
+		for (pfn = pfn_begin; pfn < pfn_end; pfn++) {
+			page = pfn_to_page(pfn);
+			if (page->ih_callno < oldest_callno) {
+				oldest_callno = page->ih_callno;
+				pfn_evict = pfn;
+			}
+		}
+		/* 3-2: Find oldest page in level 2 buckets */
+		for (i = 0; i < d; i++) {
+			pfn_begin = ihkmem.buckets[bucketnumbers[i]].begin_pfn + IH_LEVEL1_SIZE;
+			pfn_end = pfn_begin + IH_LEVEL2_SIZE;
+			for (pfn = pfn_begin; pfn < pfn_end; pfn++) {
+				page = pfn_to_page(pfn);
+				if (page->ih_callno < oldest_callno) {
+					oldest_callno = page->ih_callno;
+					pfn_evict = pfn;
+					bucketnumber = bucketnumbers[i];
+				}
+			}
+		}
+	}
+	page = pfn_to_page(pfn_evict);
+
+	if (ihkmem.util_swap == 0) {
+		ihkmem.util_swap = ihkmem.util;
+	}
+	if (ih_evict_page(page) == 0) {
+		if (oldest_callno < ih_horizon) {
+		} else {
+			/* New horizon */
+			ih_horizon = oldest_callno;
+		}
+	} else {
+		// TODO: retry
+		page->ih_callno = ih_callno_cnt++;
+		goto evict_l2;
+		panic("ICEBERG page eviction failed\n");
+	}
+
+	return bucketnumber;
+}
+
+
+int d = 6;
+struct page *get_level2_bucket(void *v, void *vma, int l1bn)
+{
+	struct page *page;
+	int bucketnumber;
+
+	bucketnumber = dchoicehashing(d, v, vma, IH_NR_BUCKETS, l1bn); //dchoice assymetric
+	if (bucketnumber == -1) {
+		page = get_level1_bucket(v, vma, &l1bn);
+		return page;
+	}
+	page = ih_rmqueue(&ihkmem.buckets[bucketnumber], 2, GFP_USER, 0);
+	ihkmem.buckets[bucketnumber].level2utilization++;
+	ihkmem.util++;
+	if (ihkmem.util > ihkmem.util_max) {
+		ihkmem.util_max = ihkmem.util;
+	}
+
+	return page;
+}
+
+void *ih_assign_bucket(void *addr, struct vm_area_struct *vma)
+{
+	int l1bn;
+	struct page *page = get_level1_bucket(addr, vma, &l1bn);
+
+	if(!page)
+	{
+		page = get_level2_bucket(addr, vma, l1bn);
+	}
+
+	return page;
+}
+
+
+int run_kicescand() {
+	//scanner run
+	if (!ihkmem.kicescand) {
+		memtest = current;
+		init_waitqueue_head(&ihkmem.kicescand_wait);
+		ihkmem.kicescand = kthread_create(kicescand, NULL, "kicescand");
+		if (IS_ERR(ihkmem.kicescand)) {
+			pr_err("Failed to start kicescand\n");
+			ihkmem.kicescand = NULL;
+		}
+		kthread_bind(ihkmem.kicescand,0); // bind to core: 0
+		wake_up_process(ihkmem.kicescand);
+		return ihkmem.kicescand->pid;
+	}
+	return -1;
+}
+
+
+// iceberg scanner
+
+static bool kicescand_work_requested(void) {
+	return 0;
+
+}
+
+static void frequency_update (struct freq *freq)
+{
+	freq->frequency[1] = bitmap_weight(freq->freq_bitmap, FREQ_BITMAP_SIZE) * 100;
+	freq->frequency[1] -= (60 * FREQ_BITMAP_SIZE);
+	freq->frequency[0] = (freq->frequency[0] * 70) + (freq->frequency[1] * 30 / 100);
+	freq->frequency[0] = freq->frequency[0] / 100;
+}
+
+static int kicescand(void *p)
+{
+	struct page *page;
+	unsigned long pfn;
+	int i,j;
+	unsigned long rand;
+	int ret;
+	int clearit;
+	int referenced;
+	unsigned long vm_flags;
+	allow_signal(SIGKILL);
+
+	set_freezable();
+
+	while (!kthread_should_stop()) {
+		if (wait_event_freezable_timeout(ihkmem.kicescand_wait,
+			kicescand_work_requested(),
+			msecs_to_jiffies(1000))) {
+			// requested work here
+		}
+
+		// Full scan
+		for(i = 0; i < IH_NR_BUCKETS; i++) {
+			pfn = ihkmem.buckets[i].begin_pfn;
+			for (j = 0; j < IH_BUCKET_SIZE; j++) {
+				page = pfn_to_page(pfn + j);
+				referenced = 0;
+				if (!page->ih_callno)
+					continue;
+				bitmap_shift_left(ihkmem.buckets[i].freqs[j].freq_bitmap,
+						ihkmem.buckets[i].freqs[j].freq_bitmap,
+						1, FREQ_BITMAP_SIZE);
+				clearit = 0;
+				if (ihkmem.buckets[i].freqs[j].frequency[0] >= 0) {
+					ret = get_random_bytes_arch(&rand, sizeof(unsigned long));
+					if (rand % 10 < 2) {
+						clearit = 1;
+					} else {
+						referenced = 1;
+					}
+				} else {
+					clearit = 1;
+				}
+				if (clearit) {
+					referenced = page_referenced(page, 1, 0, &vm_flags);
+				}
+				if (referenced) {
+					bitmap_set(ihkmem.buckets[i].freqs[j].freq_bitmap,0,1);
+					page->ih_callno = ih_callno_cnt;
+				} else {
+					bitmap_clear(ihkmem.buckets[i].freqs[j].freq_bitmap,0,1);
+				}
+				frequency_update(&ihkmem.buckets[i].freqs[j]);
+			}
+		}
+		if (signal_pending(current))
+			break;
+	}
+	ihkmem.kicescand = NULL;
+	do_exit(0);
+	return 0;
+}
+
diff --git a/mm/mempolicy.c b/mm/mempolicy.c
index 2c3a86502..8525613c7 100644
--- a/mm/mempolicy.c
+++ b/mm/mempolicy.c
@@ -103,6 +103,8 @@
 #include <asm/tlbflush.h>
 #include <linux/uaccess.h>
 
+#include <linux/iceberg.h>
+
 #include "internal.h"
 
 /* Internal flags */
@@ -2174,6 +2176,17 @@ alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,
 	int preferred_nid;
 	nodemask_t *nmask;
 
+	if (strncmp(current->comm, "mosaictest", 8) == 0) {
+	if (addr) {
+		pol = get_vma_policy(vma, addr);
+		nmask = policy_nodemask(gfp, pol);
+		preferred_nid = policy_node(gfp, pol, node);
+		page = __alloc_pages_nodemask_ih(gfp, order, preferred_nid, nmask, vma, addr);
+		mpol_cond_put(pol);
+		return page;
+	}
+	}
+
 	pol = get_vma_policy(vma, addr);
 
 	if (pol->mode == MPOL_INTERLEAVE) {
diff --git a/mm/page_alloc.c b/mm/page_alloc.c
index 519a60d5b..0fb6275df 100644
--- a/mm/page_alloc.c
+++ b/mm/page_alloc.c
@@ -72,6 +72,7 @@
 #include <linux/padata.h>
 #include <linux/khugepaged.h>
 #include <linux/buffer_head.h>
+#include <linux/iceberg.h>
 
 #include <asm/sections.h>
 #include <asm/tlbflush.h>
@@ -1398,6 +1399,11 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 
 			if (bulkfree_pcp_prepare(page))
 				continue;
+			if (page->ih_bucket) {
+				//list_del(&page->lru);
+				free_ih_page(page);
+				continue;
+			}
 
 			list_add_tail(&page->lru, &head);
 
@@ -1434,6 +1440,10 @@ static void free_pcppages_bulk(struct zone *zone, int count,
 
 		__free_one_page(page, page_to_pfn(page), zone, 0, mt, FPI_NONE);
 		trace_mm_page_pcpu_drain(page, 0, mt);
+		if (page->ih_bucket) {
+			list_del(&page->lru);
+			free_ih_page(page);
+		}
 	}
 	spin_unlock(&zone->lock);
 }
@@ -1496,6 +1506,30 @@ static inline void init_reserved_page(unsigned long pfn)
 }
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
+void __meminit init_page_ih(unsigned long pfn)
+{
+	int nid, zid;
+	struct page *page;
+
+	nid = early_pfn_to_nid(pfn);
+
+	zid = 0;
+	page = pfn_to_page(pfn);
+	mm_zero_struct_page(page);
+	set_page_links(page, zid, nid, pfn);
+	init_page_count(page);
+	page_mapcount_reset(page);
+	page_cpupid_reset_last(page);
+	page_kasan_tag_reset(page);
+
+	INIT_LIST_HEAD(&page->lru);
+#ifdef WANT_PAGE_VIRTUAL
+	/* The shift won't overflow because ZONE_NORMAL is below 4G. */
+	if (!is_highmem_idx(zone))
+		set_page_address(page, __va(pfn << PAGE_SHIFT));
+#endif
+}
+
 /*
  * Initialised pages do not have PageReserved set. This function is
  * called for each range allocated by the bootmem allocator and
@@ -3223,6 +3257,12 @@ static void free_unref_page_commit(struct page *page, unsigned long pfn)
 	 */
 	if (migratetype >= MIGRATE_PCPTYPES) {
 		if (unlikely(is_migrate_isolate(migratetype))) {
+			if (page->ih_bucket) {
+				/* Need test */
+				list_del(&page->lru);
+				free_ih_page(page);
+				return;
+			}
 			free_one_page(zone, page, pfn, 0, migratetype,
 				      FPI_NONE);
 			return;
@@ -3231,6 +3271,10 @@ static void free_unref_page_commit(struct page *page, unsigned long pfn)
 	}
 
 	pcp = &this_cpu_ptr(zone->pageset)->pcp;
+	if (page->ih_bucket) {
+		free_ih_page(page);
+		return;
+	}
 	list_add(&page->lru, &pcp->lists[migratetype]);
 	pcp->count++;
 	if (pcp->count >= READ_ONCE(pcp->high))
@@ -3267,6 +3311,11 @@ void free_unref_page_list(struct list_head *list)
 		pfn = page_to_pfn(page);
 		if (!free_unref_page_prepare(page, pfn))
 			list_del(&page->lru);
+		if (page->ih_bucket) {
+			list_del(&page->lru);
+			free_ih_page(page);
+			continue;
+		}
 		set_page_private(page, pfn);
 	}
 
@@ -5026,6 +5075,77 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order, int preferred_nid,
 }
 EXPORT_SYMBOL(__alloc_pages_nodemask);
 
+/*
+ * Iceberg allocator function
+ */
+struct page *
+__alloc_pages_nodemask_ih(gfp_t gfp_mask, unsigned int order, int preferred_nid,
+		nodemask_t *nodemask, struct vm_area_struct *vma, unsigned long addr)
+{
+	struct page *page;
+	unsigned int alloc_flags = ALLOC_WMARK_LOW;
+	gfp_t alloc_mask; /* The gfp_t that was actually used for allocation */
+	struct alloc_context ac = { };
+
+	/*
+	 * There are several places where we assume that the order value is sane
+	 * so bail out early if the request is out of bound.
+	 */
+	if (unlikely(order >= MAX_ORDER)) {
+		WARN_ON_ONCE(!(gfp_mask & __GFP_NOWARN));
+		return NULL;
+	}
+
+	gfp_mask &= gfp_allowed_mask;
+	alloc_mask = gfp_mask;
+	if (!prepare_alloc_pages(gfp_mask, order, preferred_nid, nodemask, &ac, &alloc_mask, &alloc_flags))
+		return NULL;
+
+	/*
+	 * Forbid the first pass from falling back to types that fragment
+	 * memory until all local zones are considered.
+	 */
+	alloc_flags |= alloc_flags_nofragment(ac.preferred_zoneref->zone, gfp_mask);
+
+	/* First allocation attempt */
+	page = ih_assign_bucket((void *)addr, vma);
+	alloc_flags = alloc_flags & ~__GFP_ZERO;
+	prep_new_page(page, 0, alloc_mask, alloc_flags);
+	if (likely(page))
+		goto out;
+
+	panic("No slowpath in iceberg\n");
+	/*
+	 * Apply scoped allocation constraints. This is mainly about GFP_NOFS
+	 * resp. GFP_NOIO which has to be inherited for all allocation requests
+	 * from a particular context which has been marked by
+	 * memalloc_no{fs,io}_{save,restore}.
+	 */
+	alloc_mask = current_gfp_context(gfp_mask);
+	ac.spread_dirty_pages = false;
+
+	/*
+	 * Restore the original nodemask if it was potentially replaced with
+	 * &cpuset_current_mems_allowed to optimize the fast-path attempt.
+	 */
+	ac.nodemask = nodemask;
+
+	page = __alloc_pages_slowpath(alloc_mask, order, &ac);
+
+out:
+	if (memcg_kmem_enabled() && (gfp_mask & __GFP_ACCOUNT) && page &&
+	    unlikely(__memcg_kmem_charge_page(page, gfp_mask, order) != 0)) {
+		__free_pages(page, order);
+		page = NULL;
+	}
+
+	trace_mm_page_alloc(page, order, alloc_mask, ac.migratetype);
+
+	page->ih_callno = ih_callno_cnt++;
+	return page;
+}
+EXPORT_SYMBOL(__alloc_pages_nodemask_ih);
+
 /*
  * Common helper functions. Never use with __GFP_HIGHMEM because the returned
  * address cannot represent highmem pages. Use alloc_pages and then kmap if
@@ -7044,17 +7164,28 @@ static inline void pgdat_set_deferred_range(pg_data_t *pgdat)
 static inline void pgdat_set_deferred_range(pg_data_t *pgdat) {}
 #endif
 
+static void iceberg_area_init(unsigned long start_pfn, unsigned long end_pfn)
+{
+	//ih_init(start_pfn, end_pfn);
+}
+
 static void __init free_area_init_node(int nid)
 {
 	pg_data_t *pgdat = NODE_DATA(nid);
 	unsigned long start_pfn = 0;
 	unsigned long end_pfn = 0;
+	unsigned long ih_start_pfn = 0;
+	unsigned long ih_end_pfn = 0;
 
 	/* pg_data_t should be reset to zero when it's allocated */
 	WARN_ON(pgdat->nr_zones || pgdat->kswapd_highest_zoneidx);
 
 	get_pfn_range_for_nid(nid, &start_pfn, &end_pfn);
 
+	ih_end_pfn = end_pfn;
+	ih_start_pfn = end_pfn - 131072 + 1;
+	iceberg_area_init(ih_start_pfn, ih_end_pfn);
+
 	pgdat->node_id = nid;
 	pgdat->node_start_pfn = start_pfn;
 	pgdat->per_cpu_nodestats = NULL;
diff --git a/mm/swap_state.c b/mm/swap_state.c
index 751c1ef2f..3fe3abade 100644
--- a/mm/swap_state.c
+++ b/mm/swap_state.c
@@ -842,6 +842,7 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 	struct vma_swap_readahead ra_info = {
 		.win = 1,
 	};
+	unsigned long addr;
 
 	swap_ra_info(vmf, &ra_info);
 	if (ra_info.win == 1)
@@ -858,8 +859,10 @@ static struct page *swap_vma_readahead(swp_entry_t fentry, gfp_t gfp_mask,
 		entry = pte_to_swp_entry(pentry);
 		if (unlikely(non_swap_entry(entry)))
 			continue;
+		/* ICEBERG: readahead using a correct addr */
+		addr = vmf->address + (i - ra_info.offset) * 0x1000;
 		page = __read_swap_cache_async(entry, gfp_mask, vma,
-					       vmf->address, &page_allocated);
+					       addr, &page_allocated);
 		if (!page)
 			continue;
 		if (page_allocated) {
diff --git a/mm/vmscan.c b/mm/vmscan.c
index ad9f2adaf..328a32509 100644
--- a/mm/vmscan.c
+++ b/mm/vmscan.c
@@ -169,6 +169,9 @@ struct scan_control {
  */
 int vm_swappiness = 60;
 
+static void get_scan_count(struct lruvec *lruvec, struct scan_control *sc,
+			   unsigned long *nr);
+
 static void set_task_reclaim_state(struct task_struct *task,
 				   struct reclaim_state *rs)
 {
@@ -181,6 +184,9 @@ static void set_task_reclaim_state(struct task_struct *task,
 	task->reclaim_state = rs;
 }
 
+unsigned long ih_horizon = 0;
+unsigned long ih_callno_cnt = 0;
+
 static LIST_HEAD(shrinker_list);
 static DECLARE_RWSEM(shrinker_rwsem);
 
@@ -2166,6 +2172,59 @@ unsigned long reclaim_pages(struct list_head *page_list)
 	return nr_reclaimed;
 }
 
+int ih_evict_page(struct page *page)
+{
+	int nid = NUMA_NO_NODE;
+	unsigned int nr_reclaimed = 0;
+	LIST_HEAD(page_list);
+	struct reclaim_stat dummy_stat;
+	struct scan_control sc = {
+		.gfp_mask = GFP_KERNEL,
+		.priority = DEF_PRIORITY,
+		.may_writepage = 1,
+		.may_unmap = 1,
+		.may_swap = 1,
+		.file_is_tiny = 0,
+	};
+	unsigned long pflags;
+	int ret;
+
+	ret = isolate_lru_page(page);
+	if (ret) {
+		//dump_page(page, "page isolation failure\n");
+		return IH_PAGE_UNEVICTABLE;
+		panic("page isolation failure\n");
+	}
+
+	list_add(&page->lru, &page_list);
+	nid = page_to_nid(page);
+	ClearPageActive(page);
+
+	cond_resched();
+	psi_memstall_enter(&pflags);
+
+	nr_reclaimed += shrink_page_list(&page_list,
+					NODE_DATA(nid),
+					&sc, &dummy_stat, true);
+	/* retry after writeback */
+	if (PageWriteback(page)) {
+		wait_on_page_writeback(page);
+		nr_reclaimed += shrink_page_list(&page_list,
+						NODE_DATA(nid),
+						&sc, &dummy_stat, true);
+	}
+	psi_memstall_leave(&pflags);
+
+	/* failure */
+	if (nr_reclaimed == 0 || !list_empty(&page_list)) {
+		list_del(&page->lru);
+		putback_lru_page(page);
+		return IH_PAGE_UNEVICTABLE;
+	}
+
+	return 0;
+}
+
 static unsigned long shrink_list(enum lru_list lru, unsigned long nr_to_scan,
 				 struct lruvec *lruvec, struct scan_control *sc)
 {
diff --git a/mm/vmstat.c b/mm/vmstat.c
index f8942160f..34230daad 100644
--- a/mm/vmstat.c
+++ b/mm/vmstat.c
@@ -28,6 +28,7 @@
 #include <linux/mm_inline.h>
 #include <linux/page_ext.h>
 #include <linux/page_owner.h>
+#include <linux/iceberg.h>
 
 #include "internal.h"
 
@@ -1381,6 +1382,90 @@ static void frag_stop(struct seq_file *m, void *arg)
 {
 }
 
+static void *ih_start(struct seq_file *m, loff_t *pos)
+{
+	loff_t *spos = kmalloc(sizeof(loff_t), GFP_KERNEL);
+	if (!spos || *pos > IH_NR_BUCKETS) {
+		return NULL;
+	}
+	*spos = *pos;
+	return spos;
+}
+
+static void *ih_next(struct seq_file *m, void *arg, loff_t *pos)
+{
+	loff_t *spos = arg;
+	*pos = ++*spos;
+	if (*pos > IH_NR_BUCKETS) {
+		return NULL;
+	}
+	return spos;
+}
+
+static void ih_stop(struct seq_file *m, void *arg)
+{
+	kfree(arg);
+}
+
+static int ih_show(struct seq_file *m, void *arg)
+{
+	int i, sum_l1 = 0, sum_l2 = 0;
+	loff_t *spos = arg;
+	if (*spos < IH_NR_BUCKETS) {
+		i = *spos;
+		seq_printf(m, "%d %d %d \n", i, ihkmem.buckets[i].level1utilization, ihkmem.buckets[i].level2utilization);
+		return 0;
+	}
+	if (*spos > IH_NR_BUCKETS) {
+		return -1;
+	}
+	for (i = 0; i < IH_NR_BUCKETS; i++) {
+		sum_l1 += ihkmem.buckets[i].level1utilization;
+		sum_l2 += ihkmem.buckets[i].level2utilization;
+	}
+	seq_printf(m, "Utilization: l1:%d/%d l2:%d/%d \n", sum_l1, IH_NR_BUCKETS * IH_LEVEL1_SIZE, sum_l2, IH_NR_BUCKETS * IH_LEVEL2_SIZE);
+	i = (sum_l1 + sum_l2)*(int64_t)10000/(IH_NR_BUCKETS*IH_BUCKET_SIZE);
+	seq_printf(m, "Total %d/%d (%d.%02d%%) \n", sum_l1 + sum_l2, IH_NR_BUCKETS * IH_BUCKET_SIZE, i/100, i%100);
+
+	if (sum_l1 + sum_l2 == 0) {
+		int pid = run_kicescand();
+		seq_printf(m, "kicescand[%d] launched\n", pid);
+	}
+	return 0;
+}
+
+static const struct seq_operations iceberg_op = {
+	.start	= ih_start,
+	.next	= ih_next,
+	.stop	= ih_stop,
+	.show	= ih_show,
+};
+
+static int iu_show(struct seq_file *m, void *v) {
+	int percent = ihkmem.util_swap * (int64_t)10000 / (IH_NR_BUCKETS * IH_BUCKET_SIZE);
+	seq_printf(m, "First conflict: %d/%d (%d.%02d%%) ",
+			ihkmem.util_swap, IH_NR_BUCKETS * IH_BUCKET_SIZE,
+			percent/100, percent%100);
+	percent = ihkmem.util_max * (int64_t)10000 / (IH_NR_BUCKETS * IH_BUCKET_SIZE);
+	seq_printf(m, "Max util: %d/%d (%d.%02d%%)\n",
+			ihkmem.util_swap, IH_NR_BUCKETS * IH_BUCKET_SIZE,
+			percent/100, percent%100);
+	ihkmem.util_swap = 0;
+	ihkmem.util_max = 0;
+	return 0;
+}
+
+static int iu_open(struct inode *inode, struct file *file) {
+	return single_open(file, iu_show, NULL);
+}
+
+static const struct proc_ops iutil_ops = {
+	.proc_open = iu_open,
+	.proc_read = seq_read,
+	.proc_lseek = seq_lseek,
+	.proc_release = single_release,
+};
+
 /*
  * Walk zones in a node and print using a callback.
  * If @assert_populated is true, only use callback for zones that are populated.
@@ -2044,6 +2129,8 @@ void __init init_mm_internals(void)
 	proc_create_seq("pagetypeinfo", 0400, NULL, &pagetypeinfo_op);
 	proc_create_seq("vmstat", 0444, NULL, &vmstat_op);
 	proc_create_seq("zoneinfo", 0444, NULL, &zoneinfo_op);
+	proc_create_seq("iceberg", 0444, NULL, &iceberg_op);
+	proc_create("iutil", 0444, NULL, &iutil_ops);
 #endif
 }
 
